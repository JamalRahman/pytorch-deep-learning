{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "This notebook will cover:\n",
    "\n",
    "* Tensors in PyTorch\n",
    "* Autograd - PyTorch's built in differentiation & gradient descent engine\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[0.0059, 0.9582, 0.1936],\n        [0.5838, 0.8402, 0.3526]])\ntensor([[ 0.7758,  0.0584,  1.1555],\n        [ 0.3051,  1.7288, -0.9831]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 3)\n",
    "print(x)\n",
    "x = torch.zeros(2,3 )\n",
    "print(x)\n",
    "x = torch.rand(2,3 )\n",
    "print(x)\n",
    "x = torch.randn(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ones()` and `zeros()` are both self-explanatory.\n",
    "\n",
    "`rand()` generates flots in the range (0,1) inclusive \n",
    "\n",
    "`randn()` samples the Standard Normal Distribution (with mean 0 and variance 1).\n",
    "\n",
    "Next up: torch.empty() returns garbage data of the specified data type, torch.zeros_like(A) returns a zeros tensor of shape matching that of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0.],\n        [0., 0.]])\ntensor([[0., 0.],\n        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,2)\n",
    "print(x)\n",
    "y = torch.zeros_like(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tensor Operations\n",
    "\n",
    "We can trivially use simple matrix additions, subtractions and dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2)\n",
    "y = torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2., 2.],\n        [2., 2.]])\ntensor([[2., 2.],\n        [2., 2.]])\ntensor([[2., 2.],\n        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.mm(x,y)\n",
    "print(z)\n",
    "z = x + y  # We can use infix operators or functions\n",
    "print(z)\n",
    "z = torch.add(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inplace Operations\n",
    "\n",
    "Appending a '`_`' (underscore) to an operation makes it operate in-place rather than returning new.\n",
    "E.g. `z = torch.add_(x,y)` returns Z but also augments x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Autograd\n",
    "\n",
    "Autograd is the workhorse of PyTorch. Autograd allows us to constantly track the gradients of tensors, and automatically calculate them at every step.\n",
    "\n",
    "PyTorch cleverly contains the gradient information within the Tensor data type themselves, enabled using the parameter `.requires_grad = True`. Note that Autograd only works on tensors of type `float`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x = torch.randn(1,2,requires_grad=True)\n",
    "\n",
    "# Alternatively, we can also specify requires_grad after the fact\n",
    "\n",
    "x = np.array([1.,2.,3.])\n",
    "x = torch.from_numpy(x)\n",
    "x.requires_grad_(True) # Notice the inplace operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd tracks a tensor's gradient by recording all the operations performed on the tensor in a graph form. This graph is called a Dynamic Computational Graph (DCG). The graph stores the backwards version of what operations happen to the tensor, because backprop works backwards.\n",
    "\n",
    "Here is a simple DCG for multiplication of two tensors:\n",
    "\n",
    "\n",
    "![title](img/dcg_simple.png)\n",
    "\n",
    "\n",
    "Let's examine the elements of each node:\n",
    "\n",
    "Data: The data each variable is holding. I.e. the tensor\n",
    "\n",
    "requires_grad: If true the DCG tracks all operation history\n",
    "\n",
    "grad: The grad will be None unless the backward() function is called on *some other node* in the graph, and the gradients have to be chain ruled foward. Note the grad here is with respect to the other node. For example. If you call `z.backward()` on a node `z` which depends on `x`, then the gradient stored under node `x` will be filled in as $\\frac{\\partial z}{\\partial x}$.\n",
    "\n",
    "On turning `requires_grad = True`, PyTorch will track the reverse operations like so:\n",
    "\n",
    "![title](img/dcg_grad.png)\n",
    "\n",
    "Let's see this in action. We won't quite build a NN yet, but we'll play with the differentiation engine itself.\n",
    "\n",
    "We'll initiate variable x, and then we will define equations for y and z with respect to x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([3,2],requires_grad=True)\n",
    "\n",
    "y = x + 1\n",
    "\n",
    "z = y*y + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backward`, backprop, can only be called on scalar nodes. I.e - think of outputs to the NN that spit out a number. Backward propagates the gradients that each node in the graph with respect to the output node. I.e - Backward finds the strength & direction that each node has on the specified output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(42., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.sum(z)\n",
    "print(t)\n",
    "t.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have some output '42'. We want to find the direction & magnitude of the effect that `x` has on our output.\n",
    "PyTorch has already done this for us.\n",
    "We can simply call `x.grad` to get that strength of effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.],\n",
       "        [4., 4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually returning $\\frac{\\partial t}{\\partial x}$ for x = 1 in all dimensions\n",
    "\n",
    "### Autograd Toy Problem\n",
    "\n",
    "Let's use Autograd to learn linear regression:\n",
    "\n",
    "We will have `N` data points, and each data point the x_n maps to the y_n by the function:\n",
    "$$ y_n = 2*x_n - 5 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.1728, 0.7573, 0.2879, 0.8757, 0.6251, 0.9769, 0.9691, 0.9683, 0.6568,\n        0.1783], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs=200\n",
    "N = 10\n",
    "\n",
    "w = torch.rand([N], requires_grad=True)\n",
    "b = torch.ones([1],requires_grad=True)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    # Each epoch we start again with the data set\n",
    "    x = torch.randn([N])\n",
    "    y = torch.dot(2*torch.ones([N]), x) - 5\n",
    "\n",
    "    y_hat = torch.dot(w, x) + b\n",
    "    loss = torch.mean((y_hat - y)**2) # We use MSE \n",
    "  \n",
    "    loss.backward() # We want to find the impacts of w and b on loss\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.9939258098602295 -4.957141876220703\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(w).item(),b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### References\n",
    "\n",
    "1. https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e35f8a5e455adb2a5b6e7681bb67bd69f919eea7c7e8656afcb9a71c05f4486d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}